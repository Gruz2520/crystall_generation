{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core import Structure, Lattice\n",
    "from pymatgen.entries.computed_entries import ComputedStructureEntry\n",
    "import json \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import str, Dict, Tuple\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I process each dataset from their source file into the type I require"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexandria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(source_directory: str, destination_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Moves all files from the source directory to the destination directory.\n",
    "    If the destination directory does not exist, it will be created.\n",
    "\n",
    "    :param source_directory: Path to the source directory containing the files to be moved.\n",
    "    :param destination_directory: Path to the destination directory where files will be moved.\n",
    "    \"\"\"\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Walk through the source directory\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            # Construct the full path of the source file\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            # Construct the full path of the destination file\n",
    "            destination_file_path = os.path.join(destination_directory, file)\n",
    "\n",
    "            # Move the file from the source to the destination\n",
    "            shutil.move(source_file_path, destination_file_path)\n",
    "            print(f\"Moved file: {source_file_path} -> {destination_file_path}\")\n",
    "\n",
    "source_directory = 'alexandria\\\\1'\n",
    "destination_directory = 'alexandria\\\\'\n",
    "\n",
    "move_files(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path: str):\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    return data['entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_entry_to_cif_energy(data: Dict) -> Tuple[str, Dict]:\n",
    "    \"\"\"\n",
    "    Converts a dictionary containing structure and energy data into a CIF string and energy-related data.\n",
    "\n",
    "    :param data: A dictionary containing structure and energy information. Expected keys:\n",
    "                 - 'structure': A dictionary with 'lattice' and 'sites' data.\n",
    "                 - 'energy': The energy value of the structure.\n",
    "                 - 'correction': Energy correction value.\n",
    "                 - 'entry_id': Unique identifier for the entry.\n",
    "                 - 'parameters': Additional parameters.\n",
    "                 - 'composition': Composition of the structure.\n",
    "                 - 'data': Additional data.\n",
    "    :return: A tuple containing:\n",
    "             - CIF string representation of the structure.\n",
    "             - A dictionary with energy-related data.\n",
    "    \"\"\"\n",
    "    # Extract lattice information and create a Lattice object\n",
    "    lattice = Lattice(data['structure']['lattice']['matrix'])\n",
    "\n",
    "    # Extract species and coordinates from the structure data\n",
    "    species = []\n",
    "    coords = []\n",
    "    for site in data['structure']['sites']:\n",
    "        for specie in site['species']:\n",
    "            species.append(specie['element'])\n",
    "            coords.append(site['xyz'])\n",
    "\n",
    "    # Create a Structure object using the lattice, species, and coordinates\n",
    "    structure = Structure(lattice, species, coords)\n",
    "\n",
    "    # Create a ComputedStructureEntry object to store structure and energy data\n",
    "    entry = ComputedStructureEntry(\n",
    "        structure=structure,\n",
    "        energy=data['energy'],\n",
    "        correction=data['correction'],\n",
    "        entry_id=data['entry_id'],\n",
    "        parameters=data['parameters'],\n",
    "        composition=data['composition'],\n",
    "        data=data['data']\n",
    "    )\n",
    "\n",
    "    # Convert the structure to a CIF string\n",
    "    cif_string = structure.to(fmt='cif')\n",
    "\n",
    "    # Return the CIF string and the energy-related data\n",
    "    return cif_string, entry.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(files: list, path_to_callback: str):\n",
    "    cifs_for_dataframe = []\n",
    "    data_for_dataframe = []\n",
    "    for file in tqdm(files):\n",
    "        entrys = open_json(file)\n",
    "        cifs_for_callback = []\n",
    "        data_for_callback = []\n",
    "        \n",
    "        for entry in entrys:\n",
    "            cif, entry_data = from_entry_to_cif_energy(entry)\n",
    "            cifs_for_callback.append(cif)\n",
    "            data_for_callback.append(entry_data)\n",
    "        \n",
    "        callback_df = pd.concat([pd.DataFrame(data_for_callback), pd.DataFrame({\"cif\": cifs_for_callback})], axis=1)\n",
    "        callback_df.to_csv(path_to_callback + os.path.basename(file)[:-5] + \".csv\")\n",
    "        \n",
    "        data_for_dataframe += data_for_callback\n",
    "        cifs_for_dataframe += cifs_for_callback\n",
    "            \n",
    "    df_entry_data = pd.DataFrame(data_for_dataframe)\n",
    "    df_cif = pd.DataFrame({\"cif\": cifs_for_dataframe})\n",
    "    \n",
    "    final_df = pd.concat([df_entry_data, df_cif], axis=1)\n",
    "    \n",
    "    return final_df            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'alexandria\\\\jsons\\\\'\n",
    "\n",
    "files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexandria_dataframe = create_dataframe([\"\\\\alexandria\\convex_hull_pbe.json\"], \"alexandria\\\\callbacks\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'alexandria\\callbacks'\n",
    "output_file_path = 'alexandria\\\\alexandria_full.csv'\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)  \n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)  \n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Все файлы объединены в: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"alexandria\\\\alexandria_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.formula.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_pbe = pd.read_csv(\"alexandria/convex_hull_pbe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alex_pbe.formula.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_pbe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jarvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarvis.db.figshare import data\n",
    "\n",
    "dft_3d = data('dft_3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dft_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structure_from_data(data):\n",
    "    \"\"\"\n",
    "    Создает объект Structure из предоставленных данных.\n",
    "\n",
    "    :param data: dict, содержащий данные для создания структуры\n",
    "    :return: pymatgen Structure\n",
    "    \"\"\"\n",
    "    lattice_mat = data['lattice_mat']\n",
    "    coords = data['coords']\n",
    "    elements = data['elements']\n",
    "\n",
    "    lattice = Lattice(lattice_mat)\n",
    "\n",
    "    structure = Structure(lattice=lattice, species=elements, coords=coords)\n",
    "\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = df['atoms']\n",
    "cifs_for_dataset = []\n",
    "for atom in tqdm(atoms):\n",
    "    cifs_for_dataset.append(create_structure_from_data(atom).to_file(fmt='cif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cif = pd.DataFrame({\"cif\": cifs_for_dataset})\n",
    "final_df = pd.concat([df, df_cif], axis=1)\n",
    "final_df.to_csv('Jarvis/jarvis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis = pd.read_csv(\"Jarvis\\\\jarvis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "API = 'http://aflow.org/API/aflux/'\n",
    "MATCHBOOK = 'species(Metals)'\n",
    "DIRECTIVE = '$paging(1)'\n",
    "REQUEST = API + '?' + MATCHBOOK + ',' + DIRECTIVE\n",
    "\n",
    "try:\n",
    "    with urlopen(REQUEST) as response:\n",
    "        data = response.read().decode('utf-8')\n",
    "        if data:\n",
    "            response_json = json.loads(data)\n",
    "            print(response_json)\n",
    "        else:\n",
    "            print(\"Пустой ответ от сервера\")\n",
    "except HTTPError as e:\n",
    "    print(f\"HTTP ошибка: {e.code} - {e.reason}\")\n",
    "except URLError as e:\n",
    "    print(f\"Ошибка URL: {e.reason}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Ошибка декодирования JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perov_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov = pd.read_csv(\"perov_5\\\\perov_5_file.csv\")\n",
    "perov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carbon_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon = pd.read_csv(\"carbon_24\\\\carbon_24_file.csv\")\n",
    "carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(carbon.iloc[0].cif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mp_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = pd.read_csv(\"mp_20\\\\mp20_file.csv\")\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(mp.material_id) & set(jarvis.reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mpts_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpts = pd.read_csv(\"mpts_52\\\\mpts_52.csv\")\n",
    "mpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(mp.material_id) | set(mpts.material_id) | set(jarvis.reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's merge all correct dataset in one big dataset with union pack of columns (material_id, cif). List of datasets: alexandria_pbe, carbon_24, Jarvis, mp_20, mpts_52, perov_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis = pd.read_csv(\"../data/Jarvis/jarvis.csv\")\n",
    "mpts = pd.read_csv(\"../data/mpts_52/mpts_52.csv\")\n",
    "mp = pd.read_csv(\"../data/mp_20/mp_20.csv\")\n",
    "carbon = pd.read_csv(\"../data/carbon_24/carbon_24.csv\")\n",
    "alex_pbe = pd.read_csv(\"../data/alexandria/alexandria_pbe.csv\")\n",
    "perov = pd.read_csv(\"../data/perov_5/perov_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jarvis -> jarvis_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis_cry = jarvis[[ \"reference\", \"cif\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis_cry.columns = [\"material_id\", \"cif\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarvis_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jarvis 3 type of datasets with different prefixes\n",
    "\n",
    "- Material Project with prefix 'mp-'\n",
    "- Aflow with prefix 'auid'\n",
    "- Uniq Jarvis without prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpts_52 -> mpts_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpts_cry = mpts[[\"material_id\", \"cif\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpts_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В mpts есть кристалы, которые состоят из одного атома, это надо пофиксить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mp_20 -> mp_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_cry = mp[['material_id', \"cif\"]]\n",
    "mp_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carbon_24 -> carbon_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_cry = carbon.drop([\"energy_per_atom\", 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alexandria_pbe -> alex_pbe_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_pbe_cry = alex_pbe[[\"mat_id\", \"cif\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_pbe_cry.columns = [\"material_id\", \"cif\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_pbe_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perov_5 -> perov_cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov_cry = perov[[\"material_id\", \"cif\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for understand diff between Jarvis and perov_5 datasets let's add prefix 'p-' for perov srtuctures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov_cry['material_id'] = 'p-' + perov_cry['material_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov_cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry = pd.concat([mp_cry, mpts_cry, jarvis_cry, alex_pbe_cry, carbon_cry, perov_cry], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jarvis, mp_20 and mpts_52 have crystalls from material project, so necessary to drop duplicates of this structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry = genCry.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry = genCry.reset_index().drop(\"index\", axis=1)\n",
    "genCry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry.to_csv(\"genCry_dataset/genCry.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry = pd.read_csv(\"genCry_dataset\\\\genCry.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpts_cry have some structures with 1-2 atoms and we need to drop it & some cifs can be broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.io.cif import CifParser\n",
    "from io import StringIO\n",
    "from typing import List, Optional\n",
    "\n",
    "def filter_structures_from_dataframe(\n",
    "    dataframe: pd.DataFrame,\n",
    "    cif_column: str = \"cif\",\n",
    "    material_id_column: str = \"material_id\",\n",
    "    num_of_atoms: int = 2,\n",
    "    output_file: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters structures from a DataFrame containing CIF strings based on the number of atoms.\n",
    "    Returns a new DataFrame with only the structures that have more than the specified number of atoms.\n",
    "\n",
    "    :param dataframe: Input DataFrame containing CIF strings and material IDs.\n",
    "    :param cif_column: Name of the column in the DataFrame that contains the CIF strings. Default is \"cif\".\n",
    "    :param material_id_column: Name of the column in the DataFrame that contains the material IDs. Default is \"material_id\".\n",
    "    :param num_of_atoms: Minimum number of atoms a structure must have to be included in the result. Default is 2.\n",
    "    :param output_file: Path to the file where the filtered DataFrame will be saved. If None, the file is not saved.\n",
    "    :return: A DataFrame containing the filtered structures with their material IDs and CIF strings.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "\n",
    "    for index, row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n",
    "        mat_id = row[material_id_column]\n",
    "        cif_string = row[cif_column]\n",
    "\n",
    "        try:\n",
    "            # Convert the CIF string into a file-like object for parsing\n",
    "            cif_file_like = StringIO(cif_string)\n",
    "\n",
    "            # Parse the CIF string into a pymatgen Structure object\n",
    "            parser = CifParser(cif_file_like)\n",
    "            structure = parser.get_structures()[0]\n",
    "\n",
    "            # Check if the structure has more than the specified number of atoms\n",
    "            if len(structure) > num_of_atoms:\n",
    "                filtered_data.append({\n",
    "                    material_id_column: mat_id,\n",
    "                    cif_column: cif_string\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_data)\n",
    "    \n",
    "    if output_file:\n",
    "        filtered_df.to_csv(output_file, index=False)\n",
    "        print(f\"Filtered data saved to file: {output_file}\")\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry_filtered = filter_structures_from_dataframe(genCry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry_filtered.to_csv(\"genCry_dataset\\\\genCry_f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genCry_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
